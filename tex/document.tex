\documentclass[12pt, a4paper]{article}
\usepackage[english, serbianc]{babel}
\usepackage{authblk}
\usepackage{listings}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{amsfonts}

\usepackage{subfig}
\usepackage{float}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage[%
colorlinks=true,
pdfborder={0 0 0},
linkcolor=red
]{hyperref}

\begin{document}
\date{}
\title{HR аналитика}
\author{Коста Грујчић}
\affil{12/2017}
\maketitle

\begin{abstract}
	\noindent Циљ пројекта је истренирати логистичку регресију над подацима о предвиђању одласка запосленог из предузећа на основу великог броја параметара за сваког од њих.
\end{abstract}

\section{Увод}
	Како је у питању проблем бинарне класификације, природно је користити метрику прецизности за модел, док се успешност над појединачним класама мери односом одзива и прецизности.
	
	Приметимо да нам је од веће важности предвидети да ће неки запослени напустити предузеће него да он неће. Уколико за запосленог тврдимо да ће он напустити предузеће, његов надређени или неко из HR тима може обавити додатни разговор са њим и тиме утврдити евентуални проблем. С друге стране, уколико за некога тврдимо да ће остати, он одлуку о одласку доноси изненада. С тим у вези, грешке прве и друге врсте нису еквивалентне. Зато је пожељно максимизирати одзив предвиђања одласка запосленог. Како нам је ипак важно да и прецизност буде што је могуће већа, коначан циљ је максимизирати $F_1$ меру те класе.
	
	Надаље ћемо одлазак запосленог звати позитивном класом, јер нам је циљ предвидети је, док ћемо останак запосленог звати негативном класом.

\section{Подаци}
	Подаци који су доступни су подељени у три документа, али како је сваки запослени представљен јединственим идентификатором лако их можемо спојити у један скуп података. Тако добијени скуп података третирамо као табелу чије су колоне називи атрибута, док редове тумачимо као вишедимензионе векторе. Увидом у податке можемо закључити следеће:
	
	\begin{itemize}
		\item Колоне \textsc{Over18}, \textsc{StandardHours} и \textsc{EmployeeCount} можемо уклонити јер су једнаке за све редове у табели.
		\item Присутни су недостајући подаци у појединим колонама.
		\item Постоје нумерички и категорички атрибути.
		\item Негативна и позитивна класа нису равномерно заступљене.
	\end{itemize}
	
	Из матрице корелације (слика \ref{fig:corr}) можемо видети да велики број атрибута нису у корелацији, док и када она евидентно постоји није значајно велика. То додатно отежава одабир релеватних атрибута јер међу њима постоји нелинеарна зависност.
	
	\begin{figure}[H]
		\centering
		\includegraphics[height=11cm]{graphics/correlation.pdf}
		\caption{матрица корелације нумеричких атрибута}
		\label{fig:corr}
	\end{figure}

\section{Приступ}
	Како је у питању класификациони проблем са две класе, применићемо логистичку регресију. Подаци који су нам на располагању садрже велики број атрибута од којих су неки очекивано ирелевантни. Будући да логистичка регресија не може бити директно примењена на категоричке атрибуте, потребно је извршити њихову трансформацију. Зато ћемо описати одабир својстава и начин трансформисања категоричих атрибута.
	
	Оба поменута проблема се могу решавати независно, али и заједно, те ћемо описати оба поступка и упоредити их. Надаље ћемо их редом звати \textit{експлицитни} и \textit{имплицитни} приступ према томе како одређују важност атрибута.
	
	\subsection{Увођење помоћних атрибута и експлицитно одстрањивање ирелевантних атрибута}
		У овом приступу је категоричке атрибуте неопходно пресликати у нумеричке. У ту сврху се такви атрибути пресликавају у ретке векторе који на само једном месту имају јединицу, док су свуда остало нуле. Димензија тог вектора је једнака броју вредности које одговарајући категорички атрибут може имати.
	
		Конкретно, ако категорички атрибут има свега две могуће вредности, тада се једна од њих представља вектором $(1, 0)$, а друга $(0, 1)$.
	
		Овакав приступ не уводи никакав поредак међу векторима јер они чине ортонормирану базу и то је управо оно што желимо. Међутим, имплицитно је индукована линеарна зависност атрибута, јер важи $\sum_{i} v_i = \textbf{1}$. Због тога се један од вектора мења нула вектором.
	
		Применом описаног поступка, добијамо $40$ нумеричких атрибута. Њихов број је очигледно превелик и да би се убрзао процес обучавања регресионог модела неке је потребно одстранити. Зато се на основу тренинг скупа, обучава неколико модела који могу проценити релевантност атрибута (попут насумичне шуме одлучивања или метода потпорних вектора). Уколико неки од тих модела одређени атрибут прогласи релевантним, додељује му се један глас. Бирају се сви атрибути са бар $3$ гласа. На тај начин смо преполовили број атрибута. Напомињемо да су недостајуће вредности замењене медијаном. Изгласане атрибуте можемо видети у табели \ref{table:votes}.

		\begin{figure}[!h]
			\makebox[1 \textwidth][c]{
				\resizebox{1.3 \textwidth}{!}{
					\begin{tabular}{SSSSSSSS}
						\toprule
						{} & {атрибут} & {$WOE$} & {$RF$} & {$ETC$} & {$\chi^2$} & {$L_1$} & {гласови}\\ \midrule
						{1} & \texttt{YearsAtCompany} & {1} & {1} & {1} & {1} & {1} & {5} \\
						{2} & \texttt{JobSatisfaction} & {1} & {1} & {1} & {1} & {1} & {5} \\
						{3} & \texttt{YearsSinceLastPromotion} & {1} & {1} & {1} & {1} & {1} & {5} \\
						{4} & \texttt{TotalWorkingYears} & {1} & {1} & {1} & {1} & {1} & {5} \\
						{5} & \texttt{BusinessTravel\_Travel\_Frequently} & {1} & {1} & {1} & {1} & {1} & {5} \\
						{6} & \texttt{WorkLifeBalance} & {1} & {1} & {1} & {1} & {1} & {5} \\
						{7} & \texttt{EnvironmentSatisfaction} & {1} & {1} & {1} & {1} & {1} & {5} \\
						{8} & \texttt{YearsWithcurrManager} & {1} & {1} & {1} & {1} & {1} & {5} \\
						{9} & \texttt{MaritalStatus\_Single} & {1} & {1} & {1} & {1} & {1} & {5} \\
						{10} & \texttt{Age} & {1} & {1} & {1} & {1} & {1} & {5} \\
						{11} & \texttt{NumCompaniesWorked} & {1} & {1} & {1} & {0} & {1} & {4} \\
						{12} & \texttt{TrainingTimeLastYear} & {1} & {1} & {1} & {0} & {1} & {4} \\
						{13} & \texttt{Education} & {1} & {1} & {1} & {0} & {1} & {4} \\
						{14} & \texttt{JobInvolvment} & {1} & {1} & {1} & {0} & {0} & {3} \\
						{15} & \texttt{DistanceFromHome} & {0} & {1} & {1} & {0} & {1} & {3} \\
						{16} & \texttt{PercentSalaryHike} & {0} & {1} & {1} & {0} & {1} & {3} \\
						{17} & \texttt{StockOptionLevel} & {1} & {1} & {1} & {0} & {0} & {3} \\
						{18} & \texttt{MonthlyIncome} & {0} & {1} & {1} & {0} & {1} & {3} \\
						{19} & \texttt{MaritalStatus\_Married} & {1} & {0} & {0} & {1} & {1} & {3} \\
						{20} & \texttt{JobRole\_Manufacturing Director} & {1} & {0} & {0} & {1} & {1} & {3} \\
					\end{tabular}
				}
			}
		\caption{приказ гласова одабраних атрибута}
		\label{table:votes}
		\end{figure}


		Потом се врши обучавање логистичке регресије. Како је потребно контролисати прилагођеност подацима, неопходно је одредити регуларизациони параметар регресионог модела. Како поменути параметар може узети вредност из непребројивог скупа, потребно је вршити узорковање или дискретизацију простора претраге. Опредељујемо се за први приступ и користимо \texttt{BayesSearchCV} из пакета \texttt{sklearn\footnote{Како се на овај пакет често реферише, убудуће то неће бити навођено}}. Претпоставља се $\log$-равномерна расподела параметра (у пакету \texttt{sklearn} се зове $C$). То значи да ће се параметар $C$ узорковати из поменуте расподеле и чувати онај који је најбољи у односу на задату метрику. Изводи се $300$ симулација над искључиво тренинг скупом над којим се врши унакрсна валидација ради мерења ваљаности тако изабраног параметра. Бира се параметар за који модел логистичке регресије има највећу површину испод $ROC$ криве. Водимо рачуна да се врши стратификована унакрсна валидација јер су класе неравномерно заступљене. Унакрсну валидацију лако изводимо употребом \texttt{StratifiedKFold} за $k=5$.
	
		Коначно, тако оптимизован \texttt{LogisticRegression} модел се тренира над тренинг скупом кроз $100$ итерација или до конвергенције. Како постоји изражена неуравнотеженост класа, потребно их је балансирати у регресионом моделу што постоји као могућност у поменутом пакету.
	
	\subsection{Имплицитно одстрањивање атрибута без увођења додатних атрибута}
		У претходно описаном поступку, релевантност атрибута је диктирана као хиперпараметар. Као алтернативан приступ, применићемо метод \textit{weight of evidence} који не захтева елиминацију недостајућих вредности, експлицитно мењање категоричких атрибута нумеричким као ни експлицитно бирање релевантних атрибута.
		
		Основна идеја је подела атрибута у групе којима се придружује вредност на основу односа заступљености циљне променљиве у њима. Такву вредност зовемо $WOE$ вредност. Затим се она додатно користи за пондерисање релативне заступљености позитивне класе чиме се добија \textit{нформациона вредност} (скраћено $IV$) на основу које се може проценити релевантност атрибута.
		
		Дајемо формално извођење поменутог метода. Нека је атрибут $X_j$ разврстан у групе $B_1, ..., B_n$ и нека је $Y$ бинарна случајна величина циља. Вредност
		
		\begin{equation}
			\log\frac{P(X_j \in B_i \mid Y=1)}{P(X_j \in B_i \mid Y=0)}
		\end{equation}
		
		\noindent означавамо са $WOE_{i,j}$. Можемо је оценити директно из узорка. Потом, информациону вредност атрибута $X_j$ дефинишемо као
		 
		\begin{equation}
			IV_j = \sum_{i=1}^{n}(P(X_j \in B_i \mid Y=1) - P(X_j \in B_i \mid Y=0)) \times WOE_{i,j}.
		\end{equation}
		
		Број група се унапред фиксира и најчешће износи $20$. Групе коje имају сличну $WOE$ вредност се могу посматрати као једна.
		
		Напомињемо да се приликом обучавања модела користи искључиво $WOE$ вредност, док је информациона вредност коришћена искључиво за приказ релевантности атрибута.
		
		На тај начин се целокупан поступак у великој мери аутоматизује, а најбитније, готово у потпуности уклања пристраност људске одлуке у коначном избору. Овај метод носи назив \texttt{WOEEncoder}.
	
		У табели \ref{table:iv} је дат приказ првих 10 најважнијих атрибута према информационој вредности. На основу информационих вредности закључујемо да првих шест атрибута има умерену предиктивну моћ, док је код осталих она мање изражена. Ни један атрибут нема превелику предиктивну моћ ($IV > 0.5$) што би указивало на аномалију у подацима.
	
		\begin{table}[h]
			\centering
			\begin{tabular}{SSS} \toprule
				{} & {атрибут} & {$IV_j$} \\ \midrule
				{1} & \texttt{YearsAtCompany}  & 0.284 \\ 
				{2} & \texttt{TotalWorkingYears}  & 0.263 \\
				{3} & \texttt{MaritalStatus\_Single} & 0.247 \\
				{4} & \texttt{Age} & 0.237 \\
				{5} & \texttt{YearsWithCurrManager} & 0.168 \\
				{6} & \texttt{EnvironmentSatisfaction} & 0.123 \\
				{7} & \texttt{BusinessTravel\_Travel\_Frequently} & 0.095 \\
				{8} & \texttt{JobSatisfaction} & 0.083 \\
				{9} & \texttt{MaritalStatus\_Divorced} & 0.069 \\
				{10} & \texttt{WorkLifeBalance} & 0.055 \\ \bottomrule
			\end{tabular}
			\caption{приказ информационих вредности}
			\label{table:iv}
		\end{table}
		
		Након трансформације атрибута поменутим поступком, врши се обучавање регресионог модела на већ описан начин.
	
%		На слици \ref{fig:c} се може видети како параметар $C$ утиче на површину испод $ROC$ криве.
%		\begin{figure}[h]
%			\centering
%			\includegraphics[width=15cm, height=6cm]{graphics/c_values.PNG}
%			\caption{вредности регуларизационог параметра применом Бајесове претраге. (\textit{лево}) већи тренинг скуп (\textit{десно}) мањи тренинг скуп}
%			\label{fig:c}
%		\end{figure}

\section{Резултати}
	У табели \ref{table:results_val} се може видети како оба приступа котирају на валидационом скупу. Валидациони скуп се састоји од $370$ негативних и $71$ позитивних инстанци.

	Имплицитни модел бирамо као бољи и вршимо евалуацију на тест скупу. Коначне резултате видимо у табели \ref{table:results_test}.

	\begin{table}[!htb]
		\begin{minipage}{.5\linewidth}
			\centering
			\begin{tabular}{SSSS} \toprule
				{} & {прецизност} & {одзив} & {$F_1$} \\ \midrule
				0  & 0.92 & 0.68 & 0.78 \\ 
				1  & 0.29  & 0.69 & 0.41 \\ \bottomrule
			\end{tabular}
			\caption*{експлицитни приступ}
		\end{minipage}%
		\begin{minipage}{.5\linewidth}
			\centering
			\begin{tabular}{SSSS} \toprule
				{} & {прецизност} & {одзив} & {$F_1$} \\ \midrule
				0  & 0.94 & 0.98 & \textbf{0.96} \\ 
				1  & 0.85  & 0.66 & \textbf{0.75} \\ \bottomrule
			\end{tabular}
			\caption*{имплицитни приступ}
		\end{minipage}
		\caption{резултати на валидационом скупу}
		\label{table:results_val}
	\end{table}
	
	Може се видети да имплицитни модел има одзив $0.77$ на позитивној класи и $F_1$ меру $0.82$. На тест скупу од $441$ инстанци, модел постиже укупну прецизност $95\%$.
	\begin{table}[h]
		\centering
		\begin{tabular}{SSSSS} \toprule
			{} & {прецизност} & {одзив} & {$F_1$} & {носач} \\ \midrule
			0  & 0.96 & 0.98 & 0.97 & 370 \\ 
			1  & 0.87  & 0.77 & 0.82 & 71 \\ \bottomrule
		\end{tabular}
		\caption{резултати на тест скупу}
		\label{table:results_test}
	\end{table}

\section{Закључак}
	На основу приказаних резултата, имплицитни приступ се показао као бољи. Исход је очекиван имајући у виду напреднији начин одређивања релевантности и коришћења те информације у оквиру самог модела. Чак и ако експлицитни приступ обезбеђује сличне карактеристике, моделу се ни једног тренутка не даје информација о важности атрибута већ се то мора закључити на основу сирових података.
	
	Постигнути су резултати који имају одличан одзив и $F_1$ меру над позитивном класом, што је и био циљ. За очекивати је да би успешност модела била већа када би скуп података био већи.

\section{Имплементација}
	Сав к\^{о}д се може наћи на страници GitHub \hyperref{http://www.github.com/4eyes4u/HRAnalytics}{category}{name}{репозиторијума}.

\section{Додатак}
	\subsection{Изгласавање атрибута}
		Модели коришћени за изгласавање су:
		\begin{itemize}
			\item $WOE$ -- \texttt{WOEEncoder}
			\item $RF$ -- \texttt{RandomForestClassifier}
			\item $ETC$ -- \texttt{ExtraTreesClassifier}
			\item $\chi^2$ -- \texttt{chi2}
			\item $L_1$ -- \texttt{LinearSVC}
		\end{itemize}

\end{document}